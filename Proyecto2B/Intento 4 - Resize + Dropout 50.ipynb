{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "rpVgSST_Y455",
    "outputId": "aa3258c9-eacb-4099-a711-bba9fd4b507d"
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsvzwGy-bWrT"
   },
   "source": [
    "# Clonamos el repositorio para obtener el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "DWYwSH-3Z97s",
    "outputId": "5dfdbbb1-6cc9-4aae-c779-3db6edbd4e34"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/joanby/deeplearning-az.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "w-fLk6oobhF2",
    "outputId": "0c80eec8-91fa-4455-be82-940db7c2c4e0"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "train_ds_path  ='C:/Users/Usuario/Documents/Master/Aprendizaje Profundo/Udemy/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/training_set'\n",
    "test_ds_path   ='C:/Users/Usuario/Documents/Master/Aprendizaje Profundo/Udemy/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/test_set'\n",
    "cat_or_dog_path='C:/Users/Usuario/Documents/Master/Aprendizaje Profundo/Udemy/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/single_prediction/cat_or_dog_1.jpg'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kj7yYesKbctW"
   },
   "source": [
    "# Parte 1 - Construir el modelo de CNN\n",
    "\n",
    "# Importar las librerías y paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBacLjT1Y81H"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "\"\"\"\n",
    "Necesitamos la capa de olvido para eviter el sobre-entrenamiento\n",
    "\"\"\"\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtczsRskbj2F"
   },
   "source": [
    "# Inicializar la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQL9sTJ9Y_Vc"
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUbAgrrqbpsY"
   },
   "source": [
    "# Paso 1 - Convolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kYdJsjsZCR8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "frame_size = (  ,  )\n",
    "\n",
    "esta dupla nos permitirá parametrizar la resolución\n",
    "de entrada de las imágenes\n",
    "\"\"\"\n",
    "frame_size = (96, 96)\n",
    "\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (*frame_size, 3), activation = \"relu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBFya76KbsKw"
   },
   "source": [
    "# Paso 2 - Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jty3bSFYZD98"
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgJ4s9fSb8bm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Una segunda capa de convolución y max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkVuO25YZHtg"
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0rd5DVFZIWb"
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfacNAkPb_T-"
   },
   "source": [
    "# Paso 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWl9_DHxZJZD"
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ksz0Q4WGcB77"
   },
   "source": [
    "# Paso 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQnTQSjyZKXs"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "\"\"\"\n",
    "classifier.add(Dropout(  ))\n",
    "\n",
    "Añadimos esta capa de olvido para evitar el\n",
    "sobre-entrenamiento que hemos detectado en la\n",
    "versión anterior\n",
    "\"\"\"\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7INHvHmcFdL"
   },
   "source": [
    "# Compilar la CNN\n",
    "# Como va a ser entrenada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uc7pBop7ZLeN"
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", \n",
    "                   loss = \"binary_crossentropy\", \n",
    "                   metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hits0FnccMjr"
   },
   "source": [
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "BYgCwVDFZMrU",
    "outputId": "641d711f-0723-447f-811b-1c1aaa639274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 54s 207ms/step - loss: 0.6911 - accuracy: 0.5565 - val_loss: 0.6517 - val_accuracy: 0.6160\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 0.6400 - accuracy: 0.6364 - val_loss: 0.6526 - val_accuracy: 0.6400\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 52s 206ms/step - loss: 0.5984 - accuracy: 0.6782 - val_loss: 0.5599 - val_accuracy: 0.7240\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 49s 196ms/step - loss: 0.5591 - accuracy: 0.7122 - val_loss: 0.5411 - val_accuracy: 0.7300\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 0.5437 - accuracy: 0.7237 - val_loss: 0.5422 - val_accuracy: 0.7335\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 51s 204ms/step - loss: 0.5214 - accuracy: 0.7396 - val_loss: 0.5057 - val_accuracy: 0.7610\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 51s 204ms/step - loss: 0.5075 - accuracy: 0.7449 - val_loss: 0.4817 - val_accuracy: 0.7685\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 0.4884 - accuracy: 0.7663 - val_loss: 0.4728 - val_accuracy: 0.7805\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 0.4625 - accuracy: 0.7860 - val_loss: 0.4794 - val_accuracy: 0.7770\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 51s 204ms/step - loss: 0.4537 - accuracy: 0.7879 - val_loss: 0.4795 - val_accuracy: 0.7625\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 51s 201ms/step - loss: 0.4430 - accuracy: 0.7945 - val_loss: 0.4789 - val_accuracy: 0.7725\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 0.4325 - accuracy: 0.7916 - val_loss: 0.4886 - val_accuracy: 0.7775\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 0.4260 - accuracy: 0.8046 - val_loss: 0.4675 - val_accuracy: 0.7840\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 50s 200ms/step - loss: 0.4124 - accuracy: 0.8119 - val_loss: 0.4758 - val_accuracy: 0.7690\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 51s 201ms/step - loss: 0.4226 - accuracy: 0.8084 - val_loss: 0.4378 - val_accuracy: 0.7980\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.3971 - accuracy: 0.8220 - val_loss: 0.4670 - val_accuracy: 0.7905\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.3982 - accuracy: 0.8246 - val_loss: 0.4691 - val_accuracy: 0.7880\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 0.3842 - accuracy: 0.8223 - val_loss: 0.4831 - val_accuracy: 0.7975\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 0.3854 - accuracy: 0.8216 - val_loss: 0.4579 - val_accuracy: 0.8045\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.3628 - accuracy: 0.8461 - val_loss: 0.4684 - val_accuracy: 0.7900\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.3634 - accuracy: 0.8340 - val_loss: 0.4424 - val_accuracy: 0.8120\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 52s 206ms/step - loss: 0.3500 - accuracy: 0.8450 - val_loss: 0.4514 - val_accuracy: 0.8170ccuracy: 0.84\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 0.3387 - accuracy: 0.8525 - val_loss: 0.4704 - val_accuracy: 0.8040\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 0.3247 - accuracy: 0.8583 - val_loss: 0.4691 - val_accuracy: 0.8090\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 0.3315 - accuracy: 0.8529 - val_loss: 0.5011 - val_accuracy: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c5fe045fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\"\"\"\n",
    "batch_size=32\n",
    "\n",
    "32 es el valor por defecto que usaría\n",
    "model.fit_generator, aunque aquí lo debemos\n",
    "especificar en los generadores en lugar de\n",
    "en la llamada al bucle de entrenamiento\n",
    "\"\"\"\n",
    "batch_size=32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory(train_ds_path,\n",
    "                                                     target_size=frame_size,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory(test_ds_path,\n",
    "                                                   target_size=frame_size,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode='binary')\n",
    "\"\"\"\n",
    "steps_per_epoch=8000/batchsize\n",
    "validation_steps=2000/batch_size\n",
    "\n",
    "a partir de keras 2.2.0 la función \n",
    "fit_generator queda en proceso de obsolencencia \n",
    "(programada ;-) pero sigue funcionando si \n",
    "ajustamos el número de steps, dividiéndolo \n",
    "por el tamaño de lote\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "workers=4\n",
    "\n",
    "si pedimos más de un proceso para el generador de \n",
    "imágenes, el rendimiento de las pruebas mejora un poco\n",
    "\"\"\"\n",
    "\n",
    "classifier.fit_generator(generator=training_dataset,\n",
    "                         steps_per_epoch=8000/batch_size,\n",
    "                         epochs=25,\n",
    "                         validation_data=testing_dataset,\n",
    "                         validation_steps=2000/batch_size,\n",
    "                         workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comentario a los resultados y propuesta de mejora\n",
    "\n",
    "## Resultado\n",
    "\n",
    "### Arquitectura original \n",
    "Epochs: 25,\n",
    "loss: 0.2710 - **accuracy: 0.8874** - val_loss: 0.5012 - **val_accuracy: 0.80**\n",
    "\n",
    "### Arquitectura 2 \\[añadida capa Dropout(0.2)\\]\n",
    "Epochs: 25, \n",
    "loss: 0.3350 - **accuracy: 0.8510** - val_loss: 0.4406 - **val_accuracy: 0.8105**\n",
    "\n",
    "### Arquitectura 3 \\[cambio a 96x96 píxeles y Dropout(0.3)\\]\n",
    "Epochs: 25,\n",
    "loss: 0.2813 - **accuracy: 0.8836** - val_loss: 0.6156 - **val_accuracy: 0.7655**\n",
    "\n",
    "### Arquitectura 4 \\[96x96 píxeles y Dropout(0.5)\\]\n",
    "Epochs: 25,\n",
    "loss: 0.3315 - **accuracy: 0.8529** - val_loss: 0.5011 - **val_accuracy: 0.7880**\n",
    "\n",
    "## Comentario\n",
    "Con el aumento del parámetro de olvido hasta 0.5,\n",
    "la divergencia entre las precisiones de entrenamiento\n",
    "y validación ha mejorado.\n",
    "Si la evolución descendente hasta la epoch 25 de la función de pérdida\n",
    "sugiere que aumentemos el número de epoch de entrenamiento.\n",
    "\n",
    "## Propuesta de mejora\n",
    "Una nueva ejecución con 100 epochs debería ser suficiente para\n",
    "confirmar o descartar esta propuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwHuYDWfcPEZ"
   },
   "source": [
    "# Parte 3 - Cómo hacer nuevas predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7k7KG9SZOjL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(cat_or_dog_path, target_size = frame_size)\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "\n",
    "print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
